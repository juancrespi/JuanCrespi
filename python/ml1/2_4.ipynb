{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/images/EscUpmPolit_p.gif \"UPM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Notes for Learning Intelligent Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Department of Telematic Engineering Systems, Universidad Politécnica de Madrid, © Carlos A. Iglesias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Introduction to Machine Learning](2_0_0_Intro_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Preprocessing](#Preprocessing)\n",
    "* [Training set and Test set](#Training-set-and-Test-set)\n",
    "* [Preprocessing](#Preprocessing)\n",
    "* [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to learn how to split the dataset into a training and a test datasets and then preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common practice in machine learning to evaluate an algorithm is to split the data at hand into two sets, one that we call the **training set** on which we learn data properties and one that we call the **testing set** on which we test these properties. \n",
    "\n",
    "We are going to use *scikit-learn* to split the data into random training and testing sets. We follow the ratio 75% for training and 25% for testing. We use `random_state` to ensure that the result is always the same and it is reproducible. (Otherwise, we would get different training and testing sets every time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_iris, y_iris = iris.data, iris.target\n",
    "# Test set will be the 25% taken randomly\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_iris, y_iris, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4) (38, 4)\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of train and testing\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.7 2.9 4.2 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.7 3.  4.2 1.2]]\n"
     ]
    }
   ],
   "source": [
    "#Test set\n",
    "print (x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in the scikit; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "The preprocessing module further provides a utility class `StandardScaler` to compute the mean and standard deviation on a training set. Later, the same transformation will be applied on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09752318 -0.33607276  0.34628759  0.25602312]\n",
      " [ 1.06445511  0.08448757  0.45741713  0.3906456 ]\n",
      " [-1.25950146  0.29476773 -1.09839645 -1.2248242 ]\n",
      " [ 0.83205945 -0.54635292  0.56854667  0.52526808]\n",
      " [ 0.36726814 -0.96691325  1.12419438  0.3906456 ]\n",
      " [ 0.59966379  0.50504789  1.34645347  1.87149291]\n",
      " [-1.14330363  0.71532806 -0.93170214 -1.2248242 ]\n",
      " [-0.79471015  0.92560822 -1.209526   -1.09020172]\n",
      " [ 0.71586162  0.08448757  1.06862961  0.92913553]\n",
      " [ 1.29685076  0.29476773  1.17975915  1.60224795]\n",
      " [ 1.18065293  0.29476773  1.2908887   1.60224795]\n",
      " [-1.60809495 -0.1257926  -1.26509077 -1.2248242 ]\n",
      " [ 0.59966379  0.71532806  1.12419438  1.73687043]\n",
      " [ 0.36726814 -0.33607276  0.62411145  0.3906456 ]\n",
      " [ 1.29685076  0.08448757  0.84637053  1.60224795]\n",
      " [ 0.71586162 -0.33607276  0.40185236  0.25602312]\n",
      " [ 0.25107031 -0.1257926   0.67967622  0.92913553]\n",
      " [-0.67851232  0.92560822 -1.15396123 -1.2248242 ]\n",
      " [ 0.59966379  0.50504789  0.62411145  0.65989057]\n",
      " [ 1.64544425  0.29476773  1.34645347  0.92913553]\n",
      " [-1.0271058   0.08448757 -1.15396123 -1.35944668]\n",
      " [-0.09752318  1.55644871 -1.04283168 -1.09020172]\n",
      " [ 0.83205945 -0.1257926   1.23532393  1.46762546]\n",
      " [-1.14330363 -0.1257926  -1.209526   -1.35944668]\n",
      " [ 0.25107031 -1.8080339   0.79080576  0.52526808]\n",
      " [ 0.48346596 -0.54635292  0.67967622  0.92913553]\n",
      " [ 0.36726814 -0.1257926   0.56854667  0.3906456 ]\n",
      " [ 0.36726814 -0.54635292  0.23515805  0.25602312]\n",
      " [ 0.83205945 -0.1257926   0.9019353   1.1983805 ]\n",
      " [ 0.13487248 -0.1257926   0.84637053  0.92913553]\n",
      " [-0.21372101 -0.75663309  0.34628759  0.25602312]\n",
      " [ 1.06445511  0.08448757  0.62411145  0.52526808]\n",
      " [-0.21372101 -0.54635292  0.73524099  1.1983805 ]\n",
      " [ 0.71586162  0.29476773  0.95750007  1.60224795]\n",
      " [ 1.06445511  0.08448757  1.12419438  1.73687043]\n",
      " [ 1.06445511 -0.1257926   0.9019353   1.60224795]\n",
      " [ 0.01867465 -0.75663309  0.84637053  1.06375802]\n",
      " [-0.09752318 -0.1257926   0.34628759  0.12140063]]\n"
     ]
    }
   ],
   "source": [
    "# As we see, the iris dataset is now  normalized\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Feature selection](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "* [Classification probability](http://scikit-learn.org/stable/auto_examples/classification/plot_classification_probability.html)\n",
    "* [Mastering Pandas](http://proquest.safaribooksonline.com/book/programming/python/9781783981960), Femi Anthony, Packt Publishing, 2015.\n",
    "* [Matplotlib web page](http://matplotlib.org/index.html)\n",
    "* [Using matlibplot in IPython](http://ipython.readthedocs.org/en/stable/interactive/plotting.html)\n",
    "* [Seaborn Tutorial](https://stanford.edu/~mwaskom/software/seaborn/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Licences\n",
    "The notebook is freely licensed under under the [Creative Commons Attribution Share-Alike license](https://creativecommons.org/licenses/by/2.0/).  \n",
    "\n",
    "© Carlos A. Iglesias, Universidad Politécnica de Madrid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
